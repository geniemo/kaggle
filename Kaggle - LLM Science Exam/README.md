# Overview
## Description
### Goal of the Competition
OpenBookQA 데이터셋에 영감을 받아, 이 대회는 LLM이 생성한 어려운 과학 기반 질문에 참가자들이 답변하는 도전 과제를 제시했다.

당신의 작업물은 연구자들이 LLM이 스스로를 평가하는 능력과, 자원이 제한적인 환경에서 구동될 수 있는 LLM의 잠재성을 이해하는 데에 도움을 줄 것이다.
### Context
LLM의 능력이 확장됨에 따라, LLM을 스스로를 특징화하는 데 사용하는 연구 분야가 성장하고 있다. 기존의 많은 NLP 벤치마크들이 SOTA 모델들에 너무 쉬운 것으로 판명되었기 때문에, 흥미로운 연구들이 LLM이 더 강력한 모델을 테스트하기 위한 어려운 과제를 만드는 데 사용될 수 있다는 것을 보여주고 있다.

동시에 양자화(quantization)나 지식 증류(knowledge distillation) 같은 방법들이 효과적으로 언어 모델을 가볍게 하여 그 모델들을 중간 급의 하드웨어에서 돌리는 데에 쓰이고 있다. 캐글 환경은 제출이 GPU와 시간 제한에 종속적이기 때문에 이를 공부하기 위한 특별한 시각을 제공한다.

이 과제의 데이터셋은 gpt 3.5 모델에 위키피디아에서 가져온 다양한 과학 주제에 대한 텍스트 일부를 제공하고, 다지선다 질문을 만들도록 요청하고, 너무 쉬운 질문은 걸러내서 만들어졌다.

현재 캐글에서 실행되는 제일 큰 모델은 10B 개의 파라미터를 가진 모델로 추정된다. 반면에 gpt 3.5는 175B 개의 파라미터를 갖는다. 질문에 답변을 하는 모델이 이보다 10배는 규모가 큰 질문을 만든 모델에 의해 쓰여진 테스트에 잘 응답할 수 있다면 이건 아주 흥미로운 결과일 것이다. 반면에 만약 더 큰 모델이 더 작은 모델을 효과적으로 곤경에 빠뜨릴 수 있다면, 이건 LLM이 그 자체로 벤치마크하고 테스트하는 능력에 대한 중요한 시사점을 갖는다.
## Evaluation
제출은 Mean Average Precision@3(MAP@3)으로 평가된다.
$$MAP@3 = \frac{1}{U}\sum_{u=1}^U\sum_{k=1}^{min(n, 3)}P(k) \times rel(k)$$
$U$는 테스트 셋의 수이고, $P(k)$는 컷오프 $k$에서의 정밀도, $n$은 각 질문 당 예측된 답변 개수, $rel(k)$는 지표 함수로 순위 $k$에 있는 항목이 정답이면 1, 그렇지 않으면 0을 반환한다.

각 질문에 대해 올바른 정답이 한 번 등장하면, 이후 동일한 정답에 대한 추가 예측은 계산에서 제외된다.
# Data
## Dataset Description
이 대회에서의 과제는 LLM에 의해 쓰여진 다지선다 질문에 대해 답을 하는 것이다. 질문을 생성하는 과정의 세부 사항은 공개되지 않았지만, 질문 형식을 보여주고 테스트 세트의 질문 유형을 이해할 수 있게 200개의 샘플 질문과 정답을 포함하였다. 하지만 샘플 질문과 테스트 세트의 분포는 다를 수 있으므로 일반적 질문에 잘 대응할 수 있는 솔루션이 더 좋은 성능을 낼 가능성이 높다. 각 질문은 프롬프트, 5개의 선택지, 정답 으로 구성되어 있다.
## Files
- **train.csv** - answer column이 있는 200개의 질문
- **test.csv** - test set으로, 제공되는 테스트 셋은 answer가 없는 training data와 같다. 공개되지 않은 re-run 테스트 셋은 ~4000개의 다른 프롬프트들로 구성돼 있다.
- **sample_submission.csv** - 올바른 포맷의 제출 파일 예시
## Columns
- `prompt` - 요청되는 질문
- `A`, `B`, `C`, `D`, `E` - 옵션
- `answer` - 문제를 만든 LLM이 정의한 가장 정확한 답. `A`, `B`, `C`, `D`, `E` 중 하나